\newpage
\section{Limits}

Limits are a foundational concept in calculus and analysis. They describe the behavior of 
functions and sequences as the input approaches a particular value or infinity.

\subsection{Definition of the Limit}

Let \(f\) be a function defined near a point \(a\). We say:

\[
    \lim_{x \to a} f(x) = L
\]

if for every \(\varepsilon > 0\), there exists a \(\delta > 0\) such that:

\[
    0 < |x - a| < \delta \Rightarrow |f(x) - L| < \varepsilon
\]

This is the precise \emph{\(\varepsilon\)-\(\delta\) definition} of a limit.

\subsection{Limit Calculation Rules}

\emph{Sum Rule:} \(\displaystyle \lim_{x \to a} (f(x) + g(x)) = \lim_{x \to a} f(x) + \lim_{x \to a} g(x)\)

\emph{Difference Rule:} \(\displaystyle \lim_{x \to a} (f(x) - g(x)) = \lim_{x \to a} f(x) - \lim_{x \to a} g(x)\)

\emph{Product Rule:} \(\displaystyle \lim_{x \to a} (f(x)g(x)) = \lim_{x \to a} f(x) \cdot \lim_{x \to a} g(x)\)

\emph{Quotient Rule:} \(\displaystyle \lim_{x \to a} \frac{f(x)}{g(x)} = \frac{\lim f(x)}{\lim g(x)}\) if \(\lim g(x) \ne 0\)

\emph{Power Rule:} \(\displaystyle \lim_{x \to a} {f(x)}^n = {(\lim_{x \to a} f(x))}^n\)

\emph{Root Rule:} \(\displaystyle \lim_{x \to a} \sqrt[n]{f(x)} = \sqrt[n]{\lim_{x \to a} f(x)}\) (when defined)

\subsection{Limits at Infinity}

\[
    \lim_{x \to \infty} f(x) = L \quad \text{means that } f(x) \text{ approaches } L \text{ as } x \to \infty
\]

\[
    \lim_{x \to \infty} \frac{1}{x} = 0, \quad \lim_{x \to \infty} e^x = \infty, \quad \lim_{x \to \infty} \frac{1}{x^2} = 0
\]

\subsection{Indeterminate Forms}

Indeterminate forms arise in limits when the expression does not directly imply a limit value:

\[
    \frac{0}{0}, \quad \frac{\infty}{\infty}, \quad 0 \cdot \infty, \quad \infty - \infty, \quad 1^\infty, \quad \infty^0, \quad 0^0
\]

\subsection{Limit of a Composite Function}

If \(\displaystyle \lim_{x \to a} f(x) = L\) and \(\displaystyle \lim_{x \to L} g(x) = g(L)\) 
(i.e., \(g\) is continuous at \(L\)), then:

\[
    \lim_{x \to a} g(f(x)) = g\left(\lim_{x \to a} f(x)\right)
\]

\subsection{Right and Left Limits}

\[
    \lim_{x \to a^-} f(x) = L_-, \quad \lim_{x \to a^+} f(x) = L_+
\]

The two-sided limit \(\displaystyle \lim_{x \to a} f(x)\) exists if and only if \(L_- = L_+\).

\subsection{L’Hôpital’s Rule}

If \(\displaystyle \lim_{x \to a} f(x) = \lim_{x \to a} g(x) = 0\) or \(\pm\infty\), and \(\displaystyle \lim_{x \to a} \frac{f'(x)}{g'(x)}\) exists, then:

\[
    \lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}
\]

This rule is used to resolve indeterminate forms \(\frac{0}{0}\) and \(\frac{\infty}{\infty}\).

\subsection{Limit of a fraction}

For expressions like:

\[
    \lim_{n \to \infty} {\left( \frac{a}{b} \right)}^n
\]

\begin{itemize}

    \item If \(\frac{a}{b} < 1\): limit is \(0\)

    \item If \(\frac{a}{b} > 1\): limit diverges to \(\infty\)

\end{itemize}

\subsection{Limit of a Recursive Sequence}

Let \((a_n)\) be defined recursively:

\[
    a_{n+1} = f(a_n)
\]

If \((a_n)\) converges to \(L\) and \(f\) is continuous, then:

\[
    \lim_{n \to \infty} a_n = L \Rightarrow L = f(L)
\]

This is used to find fixed points of recursive definitions.

\subsection{Important Limits}

\begin{itemize}

    \item \(\displaystyle \lim_{x \to 0} \frac{\sin x}{x} = 1\)

    \item \(\displaystyle \lim_{x \to 0} \frac{1 - \cos x}{x^2} = \frac{1}{2}\)

    \item \(\displaystyle \lim_{x \to 0} \frac{\ln(1 + x)}{x} = 1\)

    \item \(\displaystyle \lim_{x \to 0} \frac{e^x - 1}{x} = 1\)

    \item \(\displaystyle \lim_{n \to \infty} \frac{n!}{n^n} = 0\)

    \item \(\displaystyle \lim_{n \to \infty} \sqrt[n]{a} = 1\) (for \(a > 0\))

    \item \(\displaystyle \lim_{x \to 0} \frac{1}{\cos x} = 1\)

\end{itemize}

\subsection{Squeeze Theorem (Sandwich Theorem)}

If \(g(x) \le f(x) \le h(x)\) near \(a\), and:

\[
    \lim_{x \to a} g(x) = \lim_{x \to a} h(x) = L
    \Rightarrow \lim_{x \to a} f(x) = L
\]

\textbf{Example:}

\[
    \lim_{x \to 0} x^2 \cos\left( \frac{1}{x} \right) = 0
\]

\subsection{The Number \texorpdfstring{\(e\)}{e} as a Limit}

The number \(e\) is defined as:

\[
    e = \lim_{n \to \infty} {\left(1 + \frac{1}{n} \right)}^n
\]

More generally:

\[
    \lim_{n \to \infty} {\left(1 + \frac{a}{bn} \right)}^n = e^{a/b}
\]

\textbf{Example of a limit using e}

\[
    \lim_{x \to \infty}{\left(\frac{x^2 + 1}{x^2 +2}\right)}^{x^2}
\]

\[
    \lim_{x \to \infty} {\left({\left(\frac{x^2 + 1}{x^2 +2}\right)}^{x}\right)}^x
\]

\[
    \lim_{x \to \infty} {\left({\left(\frac{1 + \frac{1}{x^2}}{1 + \frac{2}{x^2}}\right)}^{x}\right)}^x
\]

\[
    \lim_{x \to \infty} {\left(\frac{e}{e^2}\right)}^x = {\left(\frac{1}{e}\right)}^x = 0
\]

\subsection{Limits of \texorpdfstring{\(\arctan\)}{arctan}}

\begin{itemize}

    \item \(\lim_{x \to \infty} \arctan x = \frac{\pi}{2}\)

    \item \(\lim_{x \to -\infty} \arctan x = -\frac{\pi}{2}\)

    \item \(\arctan x\) is continuous and differentiable everywhere, with a horizontal asymptote at 
    \(y = \pm \frac{\pi}{2}\)

\end{itemize}

\subsection{Bolzano-Weierstraß Theorem}

Every bounded, convergent sequence in \(\Complex\) has a convergent subsequence.
More precisely the sequence \(a\) has biggest and smallest accumulation point \(h^*\) and \(h_*\)

\[
    \forall \varepsilon > 0 : h_* - \varepsilon < a_n < h^{*} + \varepsilon \text{ for all-most all } 
    b \in \Naturals
\]

\[
    h^* := \lim_{n \to \infty} \sup a_n
\]

\[
    h_* := \lim_{n \to \infty} \inf a_n
\]

\subsection{Uniqueness of the Limit}

Let \( f: D \subset \Reals \to \Reals \), and suppose \(a\) is a limit point of \( D \). Then:
If \( \lim_{x \to a} f(x) = L_1 \) and \( \lim_{x \to a} f(x) = L_2 \), then \( L_1 = L_2 \).

In other words, a function can have at most one limit at a given point. This is the \emph{uniqueness of limits}.

\textbf{Proof (by contradiction)}

Assume \( L_1 \ne L_2 \), and let \( \varepsilon = \frac{|L_1 - L_2|}{3} > 0 \).

\begin{itemize}

    \item Since \( \lim_{x \to a} f(x) = L_1 \), there exists \( \delta_1 > 0 \) such that for all \( x \in D \), \( 0 < |x - a| < \delta_1 \) implies:

    \[
        |f(x) - L_1| < \varepsilon
    \]
    
    \item Since \( \lim_{x \to a} f(x) = L_2 \), there exists \( \delta_2 > 0 \) such that for all \( x \in D \), \( 0 < |x - a| < \delta_2 \) implies:

    \[
        |f(x) - L_2| < \varepsilon
    \]

\end{itemize}

Let \( \delta = \min(\delta_1, \delta_2) \), and choose \( x \in D \) such that \( 0 < |x - a| < \delta \).

Then:

\[
    |f(x) - L_1| < \varepsilon, \quad |f(x) - L_2| < \varepsilon
\]

Using the triangle inequality:

\[
    |L_1 - L_2| = |L_1 - f(x) + f(x) - L_2| \le |f(x) - L_1| + |f(x) - L_2| < \varepsilon + \varepsilon = 2\varepsilon
\]

But we chose \( \varepsilon = \frac{|L_1 - L_2|}{3} \), so:

\[
    |L_1 - L_2| < \frac{2}{3}|L_1 - L_2| \Rightarrow \frac{1}{3}|L_1 - L_2| < 0
\]

Which is a contradiction since absolute values are always non-negative.

\textbf{Conclusion:} Our assumption was false. Hence, \( L_1 = L_2 \). The limit is unique.

\QED

\subsection{Every Convergent Sequence is a Cauchy Sequence}

Let \( (a_n) \) be a sequence in \( \Reals \). We say:

If \( (a_n) \) converges, then it is a Cauchy sequence.

A sequence \( (a_n) \) is a \emph{Cauchy sequence} if:

\[
    \forall \varepsilon > 0, \ \exists N \in \Naturals \text{ such that } \forall n, m \ge N, \quad |a_n - a_m| < \varepsilon
\]

\textbf{Proof:}

Assume \( \lim_{n \to \infty} a_n = L \). Then:

\begin{itemize}

    \item For every \( \varepsilon > 0 \), there exists \( N \in \Naturals \) such that:

    \[
        \forall n \ge N, \quad |a_n - L| < \frac{\varepsilon}{2}
    \]

    \item Now, for any \( n, m \ge N \), use the triangle inequality:

    \[
        |a_n - a_m| = |a_n - L + L - a_m| \le |a_n - L| + |a_m - L| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
    \]

\end{itemize}

\textbf{Conclusion:}  

\(|a_n - a_m| < \varepsilon \text{ for all } n, m \ge N \Rightarrow (a_n) \text{ is Cauchy}\)

This completes the proof that every convergent sequence is Cauchy.

\QED