\newpage
\section{Differential Equations}

A Differential Equation is an equation that relates a function (the dependent variable)
with a variable or multiple variables (the independent variables) and its derivative.
\vspace{\baselineskip}

They can be categorized in \emph{Ordinary} and \emph{Partial} Differential Equations.

\subsection{Ordinary Differential Equations}

An equation of the form 

\[
y^n = f(x, y, x', y', \dots, y^{n -1})
\]

is called \emph{Ordinary Explicit Differential Equations of order} \(n\)
\vspace{\baselineskip}

If \(y^n = 0\) then it is \emph{implicit}
\vspace{\baselineskip}

If the exponent of the dependent variable is 1, and it
is build like a linear equation then it is categorized as \emph{Linear}

\subsection{Initial Value Problems and Solutions}

The specification of an explicit differential equation and the values
\[
x_0,\quad y(x_0) = y_0,\quad y'(x_0) = y_1,\quad \dots,\quad y^{(n-1)}(x_0) = y_{n-1}
\]
is called an \emph{initial value problem (IVP)}.

An \emph{n}-times differentiable function \( y(x) \) that satisfies 
the explicit differential equation is called the \emph{general solution} of the 
differential equation. If it also satisfies the conditions of the initial value problem 
from the previous definition, it is called the \emph{particular solution} of the IVP\@.

\subsection{Volterra Integral}

The function \(y(x)\) is a solution of the IVP \(y' = f(x,y),\quad y(x_0) = y_0\) if and only if

\[y(x) = y_0 + \int_{x_0}^{x} f(t, y(t))dt\]

\textbf{Proof:}

\[\int_{x_0}^{x} f(t, y(t))dt = \int_{x_0}^{x} y'\]
\[y(x) = y(x_0)\]

The other way would be

\[\left(\int_{x_0}^{x} f(t, y(t))dt + y_0\right)' = f(x, y(x)) = y' \]

\QED

\subsection{The Picard-Lindelöf Iteration Method}

We can approximate the desired solution using the Volterra integral equation as follows. Start with an arbitrary initial function, for example a constant — preferably the initial value:
\[
g_0(x) = y_0
\]
Then form the next approximation to the desired function \( y(x) \) as:
\[
g_{n+1}(x) = y_0 + \int_{x_0}^{x} f(t, g_n(t)) \, dt
\]

\subsection{Integration case}

To solve a ODE of the form \(y' = f(x)\) it is as easy as integrating \(f(x)\).

\subsection{Separation of Variables}

An equation \(F(x,y,y',\dots) = 0\) is a called an equation of \emph{separable variables} if it can
be written in one of the following forms:

\begin{itemize}[label=\(-\)]
    \item \(\frac{dy}{dx} = f(x) g(y)\)
    \item \(\frac{dy}{dx} = \frac{f(x)}{g(y)}\)
     \item \(\frac{dy}{dx} = \frac{g(y)}{f(x)}\)
\end{itemize}

\subsubsection{Steps to solve a ODE of separable variables}

\begin{enumerate}
    \item Solve for the derivative
    \item Check that it is indeed an ODE of separable variables
    \item Separate the variables
    \item Integrate with respect to the independent variable
\end{enumerate}

\textbf{Example:}
\vspace{\baselineskip}

Given is \((y - 2)y' = 3x - 5\)

It is an ODE of separable variable
\[y' = \frac{3x - 5}{y - 2}\]

Now we separate the variables and Integrate

\[\int (y - 2)y' dx = \int 3x - 5 dx\]

Note that \(y' = \frac{dy}{dx}\) and \( \frac{dy}{dx} dx = dy\) (only as a convenient fiction
in reality we are changing the variables) thus,

\[\int (y - 2)dy = \int 3x - 5 dx\]
\[\frac{y^2}{2} - 2y + c = \frac{3x^2}{2} - 5x + c\]

The solution we get is an implicit solution. To make it look nice solve for \(y\)
if you can.

\subsubsection{Understanding the fiction}

The previous fiction we used can explain in the following manner.
\vspace{\baselineskip}

Given \(y' = g(y)*f(x)\) separate the variables and let \(h(y) = \frac{1}{g(y)}\).

\[h(y)y' = f(x)\]

This looks kinda like the \emph{chain rule} we already know from \emph{differentiation} and
also remember that \(y\) a function of the form \(y(x)\) thus,

\[\int h(y(x)) y'(x)dx = \int f(x) dx\]

Use \emph{U-Substitution}

\[\int h(u) du = \int f(x) dx\]

\[H(y(x)) = F(x) + c\]

\[y(x) = H^{-1}(F(x) + c)\]

\QED

\subsection{Geometry of ODE}

You can imagine the plot of an ODE as a \emph{Slope field} where the slope (derivative) of a function
obeys a certain condition, this being the differential equation \(\frac{dy}{dx} = expr\).
\vspace{\baselineskip}

A point on this field is called an \emph{Initial Condition} and the curve that goes through
that point in the slope field is called an \emph{Integral curve} which a solution to the differential
equation with respect to that initial condition.


\subsection{Existence and Uniqueness}

Let \( f(x, y) \) be continuous on a rectangle \( [a, b] \times \mathbb{R} \). Then the differential equation
\[
y' = f(x, y)
\]
has at least one solution. A unique solution is only guaranteed under additional conditions.

A function \( f : \mathbb{R}^2 \to \mathbb{R} \) is called \emph{Lipschitz continuous with respect to \( y \)} if there exists a constant \( L > 0 \) such that
\[
|f(x, y_1) - f(x, y_2)| \leq L |y_1 - y_2|
\quad \text{for all } (x, y_1), (x, y_2) \in D.
\]

Let \( x_0 \in [a, b] \), and let \( f : [a, b] \times \mathbb{R} \to \mathbb{R} \) be continuous and bounded, and Lipschitz continuous in \( y \). Then the initial value problem
\[
y' = f(x, y), \quad y(x_0) = y_0
\]
has a unique solution on \( [a, b] \).
\vspace{\baselineskip}

Or in easier words: If \(f\) and \(\frac{\partial f}{\partial y}\) are continuous near \((x_0, y_0)\) then
there is a unique solution on an interval \(\alpha < x_0 < \beta\) to the IVP

\[y' = f(x,y), \quad y(x_0) = y_0\]

\subsection{How to deal with initial conditions}

After solving a differential equation we may have the problem that certain initial
conditions were set at the start, thus, making our general solution not specific.
\vspace{\baselineskip}

Solving this problem is pretty easy, because we just have to solve for the constant terms
in our general solution with respect to our initial conditions. Like for example \(y(0) = 1\)
we would plug 0 for all \(x\)'s in our general solution the whole equation will be set equal to 1.

\subsection{Solving DE with Substitution}

In some cases a complex function can be simplified via a substitution.

\subsubsection{Case I}

\[y' = f(ax + by(x) + c)\]

With a substitution like \(z(x) = ax + by(x) + c\) the right-hand side will become \(f(z)\)
and for the left-hand side we get \(y(x) = \frac{z(x) - ax - c}{b}\).
\vspace{\baselineskip}

Now we can substitute in the original equation, and we get

\[y'(x) = \frac{1}{b} (z'(x) - a) = f(z)\]

\[z'(x) = a + b f(z)\]

Now we can integrate and find the solution and then return to our original variables.
\vspace{\baselineskip}

\textbf{Example:}

\[y' = (x + y(x))^2\]

Here \(a = 1\), \(b = 1\), \(c = 0\) and \(f(z) = z^2\)
\vspace{\baselineskip}

\textbf{1. Substitute}
\[
z(x) = x + y(x)
\]
\[y(x) = z(x) - x\]
\[y'(x) = z'(x) - 1\]

\textbf{2. Plug in the DE}
\[y' = {(x + y(x))}^2\]
\[z'(x) - 1 = {z(x)}^2\]

\textbf{3. Integrate}

\[z'(x) = 1 + {z(x)}^2\]
\[\frac{z'(x)}{1 + {z(x)}^2}\]
\[\int \frac{1}{1 + z^2} dz = \int 1 dx\]
\[\arctan (z) = x + c\]
\[z = \tan(x + c)\]

\textbf{4. Return to the original variables}

\[x + y(x) = \tan(x + c)\]
\[y(x) = \tan(x + c) - x\]

\subsubsection{Case II}

\[y' = f(\frac{y}{x})\]

For this to be valid \(z(x) = \frac{y}{x}\) our \(y(x)\) needs to look like \(y(x) = z(x)x\) and so \(y' = z(x) + z'(x)x\).
\vspace{\baselineskip}

Now we substitute in the differential equation using \(f(z) = z(x) + z'(x)x\)

\[
z'(x) = \frac{f(z) - z(x)}{x}
\]

Now we can integrate and then return to our original variables. \(y(x) = z(x)x \), where \(z(x)\) is our
result of the integration.
\vspace{\baselineskip}

\textbf{Example:}
\vspace{\baselineskip}

\[y' = \frac{y(x)}{x} + 1\]

This in converted to \(y' = f(z) = z + 1\)
\vspace{\baselineskip}

\textbf{1. Substitute}

\[z(x) = \frac{y(x)}{x}\]
\[y(x) = z(x)x\]
\[y'(x) = z(x) + z'(x)x\]

\textbf{2. Plug in the DE}

\[z(x) + z'(x)x = z(x) + 1\]

\textbf{3. Integrate}

\[z'(x)x = 1\]
\[\int z' dz = \int \frac{1}{x} dx\]
\[z = (\ln(x) + c)x\]

\subsection{Linear Differential Equations}

An ODE is called linear if all occurrences of the dependent variable and its
derivatives have an exponent of at most 1.

\[a_n(x)y^n + a_{n - 1}y^{n -1} + \cdots + a_1(x)y' + a_0(x)y = b(x)\]

If such an Equation has also the property \(b(x) = 0\) it is called \emph{Homogeneous}

\subsubsection{How to solve a Linear Differential Equation}

In this case we will focus on first Order ODE\@.
\vspace{\baselineskip}

Write it in the standard form \(y' + p(x)y = f(x)\)
\vspace{\baselineskip}

Now use \emph{Integrating Factor Method}

\subsection{Integrating Factor Method}

We are going to multiply our expression by function \(r(x)\) 

\[r(x)y' + r(x)p(x)y = r(x)f(x)\]

Now we would like to write the left side as \(\frac{d}{dx} y r(x)\) because now if we
were to integrate it would take us to the expression in the right.
\vspace{\baselineskip}

When we evaluate this new expression we get that

\[\frac{d}{dx} y r(x) = y'r(x) + r'(x)y,\]

which is not quite what we actually have, but it is close. Now note that we can say

\[r'(x) = r(x)p(x)\]

This is a differential equation of separable variables thus, we can

\[\frac{r'(x)}{r(x)} = p(x)\]

\[\int \frac{r'(x)}{r(x)}dx = \int p(x)dx\]

\[ \ln(r(x)) = \int p(x) dx\]

after using \(e\)

\[ r(x) = e^{\int p(x) dx}\]

Now we have found an \(r(x)\) and now that we know that such a function exists we can return 
to our initial problem and say

\[\frac{d}{dx}r(x)y = r(x)f(x)\]

\[
y = \frac{1}{r (x) } \int r (x) f (x) dx
\]

and 

\[r(x) = e^{\int p (x) dx}\]

\textbf{Example:}
\vspace{\baselineskip}


Given are \(y' + 4y = e^{-x}\) and \(y(0) = \frac{4}{3}\)
\vspace{\baselineskip}

Here \(4 = p(x)\) and \(e^{-x} = f(x)\)
\vspace{\baselineskip}

Now let us use the formula for \(r(x) = e^{\int p (x)}\)
\vspace{\baselineskip}

this gives us \(r(x) = e^{\int 4dx} = e^{4x}\)
\vspace{\baselineskip}

Now recall what we saw earlier

\[e^{4x}y' + e^{4x}4y = e^{4x}e^{-x} = e^{3x}\]

Our left side is just \(\frac{d}{dx} e^{4x}y\)
\vspace{\baselineskip}

Now let us complete the exercise with our last formula by integrating both sides

\[\frac{d}{dx} e^{4x}y = e^{3x}\]
\[y = \frac{1}{e^4x}\int e^{3x}\]
\[y = \frac{1}{e^4x} \frac{1}{3}e^{3x} + c\]

Which is our general solution
\vspace{\baselineskip}

To find our specific solution we just plug 0 in to our general solution that gives us
\(\frac{1}{3} + c = \frac{4}{3}\) thus, \(c = 1\)

\subsection{Homogeneous Differential Equations}

A differential equation 

\[M(x,y)dx + N(x,y)dy = 0,\]

is considered \emph{Homogeneous} if and only if
\(M(x)\) and \(N(x)\) are \emph{homogeneous}, and they have the same degree.
\vspace{\baselineskip}

The equation \(y' + f(x)y = 0\) is called \emph{homogeneous linear differential equation of first order}.

\subsubsection{Homogeneous Equation}

A function \(f(x,y)\) is called \emph{homogeneous} if and only if it can be written

\[f(xt, yt) = t^n f(x,y)\]

\textbf{Example: }

\[f(x,y) = x^3 + y^3\]
\[f(tx, ty) = {(tx)}^3 + {(ty)}^3\]
\[t^3 (x^3 + y^3) = t^3 f(x,y)\]

It is important to note that the key is that each term have a total degree of \(n\).
For example: \(x^2y + x3y^2\)

\subsubsection{Homogeneous Functions Theorem}

If \(f(x,y)\) is homogeneous of degree \(0\) in \(x\) and \(y\) then 
\(f\) is a function of \(\frac{y}{x}\)

Or \(f(x,y,y')\) is homogeneous if it can be written as \(y' = f(\frac{x}{y})\) or \(y' = f(\frac{y}{x})\)

\subsection{Solving Homogeneous DE I}

\begin{enumerate}
    \item Write in the form \(M(x,y)dx + N(x,y)dy = 0\)
    \item Check if it is homogeneous
    \item Change the variables to \(y = ux\) or \(x = uy\) depending on the situation
    \item Solve using the method of separable variables method
    \item Rewrite the answer in terms of the original variables again
\end{enumerate}

\textbf{Example:}
\vspace{\baselineskip}


Give is the function \((x -y)dx = - xdy\)
\vspace{\baselineskip}

\textbf{Step 1.} We write it in the desired form \((x - y)dx + xdy = 0\)
\vspace{\baselineskip}

\textbf{Step 2.} We notice that both are of degree \(1\). This time we skip the rigorous check.
\vspace{\baselineskip}

\textbf{Step 3.} We are going to choose the simpler function to do the change of variable 
depending on the \emph{differential!} 
\vspace{\baselineskip}

The simpler function is \(xdy\) thus, we are to change \(y\)

\[y = ux\]
\[dy = ux' + xu'\]

and 

\[u = \frac{y}{x}\]

Now substitute

\[(x - ux)dx + x(udx + xdu) = 0\]
\[xdx - u x dx + x u dx + x^{2}du = 0\]
\[(x - ux + xu)dx + x^{2}du = 0\]
\[xdx + x^{2}du = 0\]
\[xdx = - x^{2}du\]
\[dx = - xdu\]

\textbf{Step 4.} Now we can integrate both sides
\[\int \frac{1}{x}dx = \int du\]
\[\ln x + c = -u + c\]

\textbf{Step 5.} No we use \(u = \frac{y}{x}\) to return to our original variables

\[\ln x + c = -\frac{y}{x} + c\]
\[-x\ln x - xc = y\]


\subsection{Solving Homogeneous DE I}

Another method for solving this kind of equations \(y' + f(x)y = 0\)is to follow the next steps.
\vspace{\baselineskip}

\textbf{Step 1: Separate the variables}

\[
\frac{dy}{dx} = -f(x)y
\]

\textbf{Step 2: Use the separation of variables}

\[
\int \frac{\frac{dy}{dx}}{y} = \int -f(x) dx
\]

\[
\int \frac{dy}{y} = \int -f(x) dx
\]

\[
    \ln|y| = \int -f(x) dx
\]

\[
    y = C e^{\int -f(x) dx}
\]

This method works more directly.

\subsection{Inhomogeneous Equation}

The equation \(y' + f(x)y = g(x)\) is called \emph{linear inhomogeneous differential equation of first order}
 \(g(x)\) is called the \emph{distortion function}. The correspondent IVP is called
\emph{linear} IVP\@.

\subsection{Solutions of LDE}

Every linear IVP with continuous functions \(f(x)\) and \(g(x)\) plus a bounded \(f(x)\) 
has exactly one solution.
\vspace{\baselineskip}

\textbf{Proof:}

\begin{align*}
y' = -f(x)y + g(x) \text{ therefore, } |-f(x)y_1 + g(x) + -f(x)y_2 + g(x) | &= |f(x)||y_1 - y_2|\\
                                                                        &\le M |y_1 - y_2|
\end{align*}

Therefore, the function is Lipschitz continuous, thus, a solution is possible.
\QED


\subsubsection{Variation of the Constants}

Recall that for the homogeneous case \(y' + f(x)y = 0\) case we use 
\(y = c e^{\int -f(x) dx}\), but now we have \(y' + f(x)y = g(x)\), thus, let us use the following
approach

\[y = c(x) e^{\int -f(x) dx}\]

Now use the product rule to derivate

\begin{align*}
y' &= c'(x) e^{\int -f(x) dx} + c(x)e^{\int -f(x) dx}(-f(x))\\
   &= c'(x)e^{\int -f(x) dx} - f(x)y
\end{align*}

thus, 

\[y' + f(x)y = c'(x)e^{\int -f(x) dx}\]

therefore, \(g(x) = c'(x)e^{\int -f(x) dx}\)
\vspace{\baselineskip}

Then we can get a solution for \(c(x)\) from

\[
g(x) = c'(x)e^{\int -f(x) dx}
\]
\[
g(x) = c'(x)e^{-\int f(x) dx}
\]
\[
c'(x) = \frac{g(x)}{e^{-\int f(x) dx}}
\]
\[
    c'(x) = g(x)e^{\int f(x) dx}
\]

\textbf{Example: }
\vspace{\baselineskip}

Given is \(y' -\cos(x)y = x e^{\sin(x)}\) with \(y(0) = 3\).
\vspace{\baselineskip}

\textbf{1. Solve as if it were homogeneous}

\[y' -\cos(x)y = 0\]
\[y' = \cos(x)y\]
\[\frac{y'}{y} = \cos(x)\]
\[\int\frac{1}{y}dy = \int \cos(x)dx \]
\[ \ln |y|  + c= \sin(x) + c \]
\[ y = e^{\sin(x)}c \]

\textbf{2. Use \(y = c(x)e^{\sin(x)dx}\)}

\[y' = c(x)\cos(x)e^{\sin(x)} + c'(x)e^{\sin(x)}\]
\[y' = \cos(x)y + c'(x)e^{\sin(x)}\]
\[y' - \cos(x)y = c'(x)e^{\sin(x)}\]

\textbf{3. Compare with the original equation}

\[y' -\cos(x)y = x e^{\sin(x)}\]
\[y' - \cos(x)y = c'(x)e^{\sin(x)}\]

\textbf{4. Solve for \(c'(x)\)}

\[xe^{\sin(x)} = c'(x)e^{\sin(x)}\]
\[x = c'(x)\]

\textbf{5. Integrate}

\[\int x dx= \int c'(x)dx\]
\[\frac{x^2}{2} + c = c(x)\]

\textbf{6. Substitute in our approach \(y = g(x)e^{\sin(x)}\) from earlier}

\[y = \left(\frac{x^2}{2} + c\right)e^{\sin(x)}\]

\textbf{7. Solve for \(c\) using our initial condition if asked}

\[y(0) = 3\]
\[3 = \left(\frac{0^2}{2} + c\right)e^{\sin(0)}\]
\[3 = c\]


\subsection{Existence and Uniqueness Differential Equations}

For the IVP

\[y^{n} + p_{n - 1}y^{n - 1}+ \cdots + p_0 (x)y = f(x)\]

\[y(a) = b_0, y'(a) = b_1, \dots, y^{n - 1}(a) = b_{n - 1} \]

If all \(p_i\) and \(f\) are continuous on the interval \(I\) about \(a\), then
there exists a unique solution on \(I\).

\subsection{Constant Coefficients and the Superposition Theorem}

\subsubsection{Theorem of Superposition and the General Solution}

Suppose \(y_1, \dots, y_n\) solve 

\[y^{n} + p_{n - 1}y^{n - 1}+ \cdots + p_0 (x)y = 0\]

Then \(y = c_1 y_1 + \cdots + c_n y_n\) also solves the equation, and it is the 
\emph{General Solution} if and only if the \emph{Wronskian} \(W(t) \ne 0\) for some \(t_0\).


\subsection{Method of the Right Side}

Given an equation \(y' + ay = g(x)\) notice that \(a\) does not depend on \(x\).
The homogeneous equation to this case will be \(y' + ay = 0\) with a solution \(y_h\) and some
partial solution \(y_p\). Now remember from \emph{Linear Algebra} the \(general = particular + homogeneous\)
more precisely \(y = y_p + y_h\) therefore, adding a particular solution to a homogeneous does not change
the general solution of our DE.
\vspace{\baselineskip}

Now let us substitute in our differential equation.

\begin{align*}
(y_h + y_p)' + a(y_h + y_p) &= y_h' + y_p' + ay_h + ay_p\\   
&= (y_h' + ay_h) + (y_p' + ay_p) = 0 + g(x) = g(x) 
\end{align*}

\subsubsection{Steps for solving Constant Coefficients problems}
\begin{enumerate}
    \item Find the \emph{homogeneous} solution for \(y' + ay = 0\). \(y_h\) is going
    to have a free parameter.
    \item Find a partial solution for \(y' + ay = g(x)\)
    \item Use \(y = y_h + y_p\)
    \item If you have an IVP solve for the free parameter in \(y_h\)
\end{enumerate}

Now we have to find a way to solve for \(y_p\) and \(y_h\)

\subsubsection{Find the partial solution}

We will use the method called based on guessing the type of the function on the right side.
\vspace{\baselineskip}

\textbf{Example: }
\vspace{\baselineskip}

Given is the following IVP: \(y' + 2y = 2x + 13\) with \(y(0) = 8\)
\vspace{\baselineskip}

By looking carefully we see that on the right-hand side we just have a normal polynomial,
thus, we can substitute \(y = bx + c\) and derivate \(y' = b\). In the case for when we have higher
order derivative we would differentiate for each of them. 
\vspace{\baselineskip}

Now substitute

\[
b + 2(bx + c) = 2x + 13
\]

\[
2bx + b + 2c = 2x + 13
\]

Now by comparing the Coefficients with the other side 

\[2b = 2\]
\[b + 2c = 13\]

We get: \(b = 1\), \(c = 6\). And we have found a \emph{particular} solution for our DE 
\(y_p = x + 6\)

Now we have to find a \emph{homogeneous} solution with the formula we already know
\(y_h = c e^{\int f(x)dx}\) in our case \(y_h = c e^{\int 2dx} = ce^{-2x}\)

\[
y = y_h + y_p = c e^{-2x} + x + 6
\]

Now considering the condition \( y(0) = 8 \), this gives:

\[
y(0) = c + 6 = 8 \Rightarrow c = 2
\]

and we obtain the particular solution:

\[
y = 2 e^{-2x} + x + 6
\]

\subsubsection{Table of reference}
\bigskip
\begin{tabular}{|l|l|l|}
    \hline
    \textbf{Type of Forcing Function} & \textbf{Disturbance Function \( g(x) \)} & \textbf{Approach for \( y_p \)} \\
    \hline
    Constant & \( k_0 \) & \( c_0 \) \\
    \hline
    Linear & \( k_0 + k_1 x \) & \( c_0 + c_1 x \) \\
    \hline
    Polynomial & \( \sum\limits_{i=0}^{n} k_i x^i \) & \( \sum\limits_{i=0}^{n} c_i x^i \) \\
    \hline
    Exponential & \( k e^{bx}, \; b \ne a \) & \( c_0 e^{bx} \) \\
               & \( k e^{ax} \) & \( c_0 x e^{ax} \) \\
    \hline
    Trigonometric & \( k \sin(bx) + l \cos(bx) \) & \( c_0 \sin(bx) + c_1 \cos(bx) \) \\
    \hline
\end{tabular}
\vspace{\baselineskip}

It is important to point out that sometimes it is necessary to combine different types
of functions to get the solution.
\vspace{\baselineskip}

\textbf{Example:}

\[
y'' -2x' 3y = t^2  + 3e^{-t} \cos (4t)
\]

Here the approach would be

\[
y_p = (A + Bt + Ct^2) + D(e^{-t}\cos(4t)) + E(e^{-t}\sin(4t))
\]

The best way is to tackle down each of the types of functions separately.
\vspace{\baselineskip}

An alternative to way of using this method is to
\begin{enumerate}
    \item Find the homogeneous solution.
    \item Multiply both sides of the equation by it.
    \item Integrate
\end{enumerate}

This will give you the exact same result and is faster.

\subsection{Bernoulli Equation}

This is used for equations of the form 

\[
y' + P(x)y = Q(x)y^n
\]

We can not use the methods we already know for this equation because it is linear but with a
clever substitution we can turn it into a linear differential equation.
\vspace{\baselineskip}

Let us use a substitution 

\[u = y^{1 - n}\] 

then 

\[u' = (1 - n)y^{-n} y'\] 

Now this 
looks kinda similar to the original equation multiply by \(y^{-n}\) that looks like

\[
y^{-n}y' + P(x)y^{-n}y = Q(x)y^{-n}y{n}
\]

\[
y^{-n}y' + P(x)y^{1 - n} = Q(x)
\]


Notice that \(y^{-n} = \frac{u'}{(1-n)y'}\), thus, after substitution

\[
\frac{1}{1 - n}u' + P(x)u = Q(x)
\]

Now our equation is \emph{linear}, and we can use the \emph{integrating factor} method.
\vspace{\baselineskip}

\textbf{Example: }
\vspace{\baselineskip}

Given is \(y' -5y = \frac{-5}{2}xy^3\).

\[
u = y^{-2}  \text{ and } u' = -2y^{-3}y'
\]

\[
y^{-3} = \frac{u'}{-2y'}
\]

And our equation after multiplying by \(y^{-3}\)

\[
 y^{-3}y' - 5yy^{-3} = -\frac{5}{2}x 
\]

\[
y^{-3}y' - 5y^{-2} = -\frac{5}{2}x 
\]

\[
\frac{u'}{-2y'}y' - 5u = -\frac{5}{2}x 
\]

Now we can use the \emph{Integrating Factor Method}. First bring to the standard form.
\(y' + p(x)y = f(x)\)

\[
  u' + 10u = 5x 
\]

Remember that \(r(x) = e^{\int p(x)dx} = e^{\int 10 dx} = e^{10x}\) therefore, we have

\[
e^{10x}u' + 10ue^{10x} = e^{10x}5x
\]

Now let us integrate

\[
\frac{d}{dx} \left(e^{10x}u\right) = e^{10x}5x
\]

\[
Ce^{10x}u = \int e^{10x} 5x dx = 5x \frac{e^{10x}}{10} - \int e^{10x}5dx
\]

\[
Ce^{10x}u = \int e^{10x} 5x dx = 5x \frac{e^{10x}}{10} - \frac{5}{10}\int e^{10x}dx
\]

\[
Ce^{10x}u = \frac{xe^{10x}}{2} - \frac{5}{100} e^{10x} + c
\]

\[
    Ce^{10x} = \frac{x}{2} - \frac{1}{20} + \frac{c}{e^{10x}} = y^{-2}
\]


\subsection{Autonomous Equations}

An \emph{Autonomous Differential Equation} only depends on the dependent
variable \(y\).\textbf{ Example: } \(\frac{dy}{dt} = (1 + y)(1 -y)\).

\subsubsection{Equilibrium Solutions}
Values where \(f(y) = 0\) are \emph{Equilibrium Solutions}.

\subsubsection*{Asymptotically stable}
An equilibrium solution \(y = a\) is \emph{asymptotically stable} if solutions
that start near \(a\) tend towards \(a\) as \(t \to \infty\).

\subsubsection*{Asymptotically unstable}
An equilibrium solution \(y = a\) is \emph{asymptotically unstable} if solutions
that start near \(a\) leave as \(t \to \infty\)


\subsection{Linear Inhomogeneous DEs with Non-Constant Coefficients}

The method of \emph{variation of constants} is often used alongside the \emph{superposition principle}. In this method, the integration constant in the function \( c(x) \) is omitted as it is part of the particular solution.
\vspace{\baselineskip}

The superposition principle also holds for linear inhomogeneous differential equations with non-constant coefficients:

\begin{align*}
y_p' + f(x)y_p &= g(x) \quad \text{(particular solution)} \\
y_h' + f(x)y_h &= 0 \quad \text{(homogeneous solution)}
\end{align*}

For the total solution \( y = y_h + y_p \), we get:

\begin{align*}
y' + f(x)y 
&= (y_h + y_p)' + f(x)(y_h + y_p) \\
&= y_h' + y_p' + f(x)y_h + f(x)y_p \\
&= g(x)
\end{align*}

Thus, the solution can be decomposed into the general solution of the homogeneous DE and a particular solution based on the form of the right-hand side.
\vspace{\baselineskip}

\textbf{Example: }

\[y' = -\frac{y}{x} + 1\]

The homogeneous DE is \(y' = -\frac{y}{x}\) which can be solved by separating the variables.
\(y_h = \frac{c}{x}\)

Now let use \(y_p = \frac{c(x)}{x}\).

\[
y_{p}' = \frac{c'(x)x - c(x)}{x^2}
\]

Let us break down the fraction to find where to put our homogeneous solution we have already found

\[
\frac{c'(x)}{x} - \frac{c(x)}{x^2} = \frac{c'(x)}{x} + \frac{1}{x}\frac{c}{x} = \frac{c'(x)}{x} + \frac{1}{x}y
\]

and we get

\[
y_{p}' = -\frac{y}{x} + \frac{c'(x)}{x} 
\]

Here we compare with the inhomogeneous part of the original equation and get

\[
\frac{c'(x)}{x} = 1 \implies c'(x) = x
\]

then \(c(x)\) is equal to \(\frac{x^2}{2}\) and therefore, \(y_p = \frac{x^2}{x}\frac{1}{x} = \frac{x}{2}\)
\vspace{\baselineskip}

As our final result we get \(y = \frac{c}{x} + \frac{x}{2}\)

\subsection{Power Series Approach}

Another method to solve a certain type  of differential equation
is to find the function given an initial conditions. This can be better understood with an example.

\[y' = y\]
\[y(0) = 1\]

We say \(y = \sum_{n = 0}^{\infty} a_n x^n\) and \(y(0) = a_0 = 1\).

Now let us derivate you \(y\)

\[
y = \sum_{n = 0}^{\infty} a_n x^n = a_0 x^0 + a_1 x^1 + \cdots = 1 + a_1 x^1 + a_2 x^2 + \cdots
\]
\[
y' = 0  + a_1 + 2 a_2 x + 3 a_3 x^2 + \cdots 
\]

Thus, we can rewrite the sum as 

\[
y' = \sum_{n = 1}^{\infty} a_n n x^{n - 1}
\]

Starting from 1 because 0 does not contribute to the sum. After shifting the index we get

\[
y' = \sum_{n = 0}^{\infty} a_{n + 1}(n + 1)x^{n}
\]

Now we build our original DE with our new definitions for \(y'\) and \(y\)

\[
\sum_{n = 0}^{\infty} a_{n + 1}(n + 1)x^{n} = \sum_{n = 0}^{\infty} a_n x^n
\]

If we compare the coefficients we get

\[
a_n = a_{n+1}(n+1)
\]

\[
\frac{a_n}{(n + 1)} = a_{n+1}
\]


after iterating backwards we get

\[
a_{n + 1} = \frac{a_n}{n + 1} = \frac{\frac{a_{n - 1}}{(n - 1 +1)}}{n + 1} = \frac{\frac{\frac{a_{n - 2}}{(n - 1)}}{(n - 1 + 1)}}{n + 1}   
\]

\[
a_n = \frac{a_0}{(n + 1)!} = \frac{1}{(n + 1)!} = 1
\]

therefore, \(a_n = \frac{1}{n!}\) which gives us the solution

\[\sum_{n = 0}^{\infty}\frac{1}{n!}x^n = e^x\]

\subsubsection{Theorem for the Power Series of DE}

For a differential equation of the form:

\[
A(x)y^{n} + \cdots + B(x)y' + C(x)y = 0
\]

more specific after dividing by \(A(X)\)

\[
y^{n} + \cdots + P(x)y' + Q(x)y = 0,
\]

with \(x = a\) as an \emph{Ordinary Point} if \(\dots, P(x)\) and \(Q(x)\) are 
\emph{Analytic} at \(x = a\). Otherwise, a \emph{Singular Point}.
\vspace{\baselineskip}

Then this equation has \(n\) linearly independent solutions of the form

\[
y(x) = \sum_{n = 0}^{\infty} c_n {(x - a)}^n
\]

The radius of convergence is at least as large as distance to the nearest \emph{Singular Point}.

\subsection{Exact Differential Equations}

We will take a short look at functions with two inputs via implicit differentiation \(F(x,y(x)) = 0\).
We have:

\[
F_x dx + F_y dy = 0
\]
\[
p(x,y)dx + q(x,y)dy = 0
\]

after dividing by \(dx\)

\[
F_x + F_y y' = 0
\]
\[
p(x,y)dx + q(x,y)y' = 0
\]

If this is a differential then \(p_x = q_y\), and we define a DE of the form:

\[
p(x,y)dx + q(x,y)y' = 0 \text{ with } p_x = q_y,
\]

as \emph{exact}. And the \(p_x = q_y\) as \emph{Integrability-Condition}.
\vspace{\baselineskip}

By finding the \emph{potential} we can solve this kind of differential equations.
\vspace{\baselineskip}

\textbf{Example:}
\vspace{\baselineskip}

\textbf{Step 1: Solve for \(y'\)}
\begin{align*}
    (12xy + 3)dx + 6x^2dy &= 0\\
    (12xy + 3) + 6x^2 y' &= 0\\
    y' = - \frac{12xy + 3}{6x^2}
\end{align*}

\textbf{Step 2: Test the Integrability-Condition}

\[Q(x,y) = 12xy + 3\]
\[P(x,y) = 6x^2\] 
\[P_x = 12x\]
\[Q_y = 12x\]

\begin{align*}
F(x,y) &= \int (12xy + 3)dx = \int 6x^2 dy\\
       &= 6x^2 y + 3x + c(y) = 6yx^2 + c(x)
\end{align*}

Now we differentiate both sides with respect to \(y\) and this gives us

\begin{align*}
    6x^2y + 3x + c(y) &= 6yx^2 + c(x) \\
    6x^2 + c'(y) &= 6x^2 \\
           c'(y) &= 0 \\
            c(y) &= y \text{ after integrating with respect to } dy,
\end{align*}

and therefore, \(F(x,y) = 6x^2y + 3x + \hat{c} = 0\).
\vspace{\baselineskip}

\textbf{Step 3: Solve for \(y\)}

\[y = \frac{\hat{c} - 3x}{6x^2} \]

\subsection{Constant Coefficients ODE}

Consider the second-order linear differential equation with constant coefficients:
\[
a y'' + b y' + c y = 0,
\]
where \( a, b, c \in \mathbb{R} \), and \( a \neq 0 \).

We look for solutions of the form \( y = e^{rt} \). Substituting into the equation:

\begin{align*}
y &= e^{rt}, \\
y' &= r e^{rt} \\
y'' &=  r^2 e^{rt}
\end{align*}


Substituting into the original equation:
\[
ar^2 e^{rt} + bre^{rt} + ce^{rt} = 0
\]

Factoring out \( e^{rt} \) (which is never zero):
\[
e^{rt}(ar^2 + br + c) = 0 \quad \Rightarrow \quad ar^2 + br + c = 0.
\]

This is a quadratic equation in \( r \). Solving using the quadratic formula:
\[
r = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}.
\]

Suppose there are two linearly independent solution \(y_1\) and \(y_2\) to
\[
y'' + p(x)y' + q(x)y = 0,
\]

then the general solution is \(y = c_1 y_1 + c_2 y_2\).
\vspace{\baselineskip}

We analyze the nature of the roots \( r \) based on the discriminant \( D = b^2 - 4ac \).

\subsubsection{Case 1: Two Distinct Real Roots \texorpdfstring{\( (D > 0) \)}{}}

There not much not to just plug the solutions of the root.

\[y = c_1 e^{r_1 t} + c_2 e^{r_2 t}\]

\subsubsection{Case 2: Repeated Real Root \texorpdfstring{\( (D = 0) \)}{}}

For this case we need to use a trick.

\[y = c_1 e^{rt} + c_2 te^{rt}\]

Note that this could be contra intuitive but \(te^{rt}\) is a solution and is linearly independent.
For more repeated roots you can use higher powers of \(t\) \(t, t^2, t^3, \dots\).

\subsubsection{Case 3: Complex Conjugate Roots \texorpdfstring{\( (D < 0) \)}{}}

When the discriminant \( D = b^2 - 4ac < 0 \), the roots of the characteristic equation are complex:
\[
r = \alpha \pm i\beta,
\]
where \( \alpha = -\frac{b}{2a} \) and \( \beta = \frac{\sqrt{4ac - b^2}}{2a} \).

The general solution to the differential equation is:
\[
y(t) = c_1 e^{(\alpha + i\beta)t} + c_2 e^{(\alpha - i\beta)t}.
\]

To express this in real form, we use Euler’s formula:
\[
e^{i\beta t} = \cos(\beta t) + i\sin(\beta t).
\]

Now rewrite each exponential term:

\begin{align*}
e^{(\alpha + i\beta)t} &= e^{\alpha t} \cdot e^{i\beta t} = e^{\alpha t} (\cos(\beta t) + i \sin(\beta t)), \\
e^{(\alpha - i\beta)t} &= e^{\alpha t} \cdot e^{-i\beta t} = e^{\alpha t} (\cos(\beta t) - i \sin(\beta t)).
\end{align*}

Now to get rid of the imaginary part we can do the following trick

\[\frac{y_1 + y_2}{2} = e^{\alpha t}\cos\beta t\]
\[\frac{y_1 - y_2}{2i} = e^{\alpha t}\sin\beta t\]

This might feel wrong, but these are in fact linear combinations of our original solution,
and they are also linearly independent, because of that we can just use them for our general solution.

\[y = c_1 e^{\alpha t}\cos\beta t + c_2  e^{\alpha t}\sin\beta t\]

\textbf{Example:}
\vspace{\baselineskip}

Given is \(y''\,' + y' = 0\).

\[y = e^{rt}\]

\begin{align*}
r^3 + r &= 0\\
r(r^2 + 1) &= 0\\
r(r + i)(r - i) &= 0
\end{align*}

Now \(r_1 = 0 \quad r_2 = -i \quad r_3 = i\)

\[y = c_1 e^{0t} + c_2 e^{-it} + c_3 e^{it}\]

\[y = c_1 + c_2 \cos t + c_3 \sin t\]

\subsection{The Wronskian}

Let \( y_1, y_2, \ldots, y_n \) be \( n \) functions that are at least 
\( (n-1) \)-times differentiable on some interval. The \emph{Wronskian} of these 
functions is defined as:

\[
W(y_1, y_2, \ldots, y_n)(x) =
\begin{vmatrix}
y_1(x) & y_2(x) & \cdots & y_n(x) \\
y_1'(x) & y_2'(x) & \cdots & y_n'(x) \\
\vdots & \vdots & \ddots & \vdots \\
y_1^{(n-1)}(x) & y_2^{(n-1)}(x) & \cdots & y_n^{(n-1)}(x)
\end{vmatrix}
\]

If \(W(A) \ne 0\) then they are linearly independent.


\subsection{Linearity Property}

Let \( y_p^{(1)} \) be a particular solution of  
\[
y'' + ay' + by = g_1(x)
\]  
and \( y_p^{(2)} \) a particular solution of  
\[
y'' + ay' + by = g_2(x),
\]  
then  
\[
y_p = y_p^{(1)} + y_p^{(2)}
\]  
is a particular solution of  
\[
y'' + ay' + by = g_1(x) + g_2(x).
\]  

\subsection{Variation of Parameters}

For a linear differential equation of the form

\[
y'' + p(x)y' + q(x)y = f(x)
\]

The homogeneous equation has a solution of the linearly independent \(y_1\) and \(y_2\)
of the form \(y = u_1 y_1 + u_2 y_2\). We can guess the functions \(y_1\) and \(y_2\).
\vspace{\baselineskip}

So, for finding a solution of these types of equations we use the following formulas.

\[
u_1 = - \int \frac{y_2 g}{y_1 y_{2}' - y_2 y_{1}'} dx
\]


\[
u_2 = - \int \frac{y_1 g}{y_1 y_{2}' - y_2 y_{1}'} dx
\]

These formulas come from:
\vspace{\baselineskip}

\textbf{Differentiating two times:}

\[
y = u_1 y_1 + u_2 y_2
\]

\[
y' = u_1 y_{1}' + y_1 u_{1}' +  u_2 y_{2}' + y{2} u_{2}'
\]

Here we can add another constraint that \(y_1 u_{1}' + y{2} u_{2}' = 0\). This allows us to
simplify the expression to:

\[
y' = u_1 y_{1}' +  u_2 y_{2}'
\]

Now we can derivate one more time:

\[
y'' =  u_1 y_{1}'' + y_{1}' u_{1}'  + u_2 y_{2}'' + y_{2}' u_{2}'
\]


\textbf{Substitute in the original equation}

\[
y'' + p(x)y' + q(x)y = f(x)
\]

\[
 u_1 y_{1}'' + y_{1}' u_{1}'  + u_2 y_{2}'' + y_{2}' u_{2}' + p(x)(u_1 y_{1}' +  u_2 y_{2}') + q(x)(u_1 y_1 + u_2 y_2) = g(x)  
\]

\[
 u_1 y_{1}'' + y_{1}' u_{1}'  + u_2 y_{2}'' + y_{2}' u_{2}' + p(x)(u_1 y_{1}') + p(x)(u_2 y_{2}') + q(x)(u_1 y_1) + q(x)(u_2 y_2) = g(x)  
\]

\[
 u_1 y_{1}'' + y_{1}' u_{1}'  + u_2 y_{2}'' + y_{2}' u_{2}' + p(x)(u_1 y_{1}') + p(x)(u_2 y_{2}') + q(x)(u_1 y_1) + q(x)(u_2 y_2) = g(x)  
\]

With non-generic function a lot of the terms would cancel out. After that we add multiply by
\(y_1\) one of the rows and \(y_2\) so that we can cancel them in a row operation. Which will left us
with only two integrals to solve.






