\newpage
\section{Eigenvectors and Eigenvalues}

\emph{Eigenvector:}

An eigenvector of a square matrix \(A\) is a non-zero vector \(\vec{v}\) that, when multiplied by 
\(A\), results in a vector that is a scalar multiple of itself. In other words, the direction of the 
vector \(\mathbf{v}\) remains unchanged (up to scaling) when the linear transformation represented by 
\(A\) is applied to it.
\vspace{\baselineskip}

\emph{Eigenvalue:}

The scalar multiple, denoted by \(\lambda\), is called the eigenvalue associated with the eigenvector 
\(\mathbf{v}\). It represents the factor by which the eigenvector is scaled when transformed by 
the matrix \(A\).
\vspace{\baselineskip}

Mathematically, the relationship between a square matrix \(A\), an eigenvector \(\vec{v}\), 
and its corresponding eigenvalue \(\lambda\) is expressed by the following equation:

\[
    A\vec{v} = \lambda\vec{v}
\]

Where \(A = f: V \to V\) is an \emph{endomorphism}, \(\vec{v} \ne \vec{0}\) and  \(\lambda \in \Complex\).
\vspace{\baselineskip}

The way we find a method to find them is by manipulating the equation above

\begin{align*}    
    A\vec{v} &= \lambda\vec{v}\\
    A\vec{v} - \lambda\vec{v} &= \vec{0}\\
    A\vec{v} - \lambda I \vec{v} &= \vec{0}\\
    (A\vec{v} - \lambda I) \vec{v} &= \vec{0}
\end{align*}

Here note that if \((A\vec{v} - \lambda I)\) were invertible then we could multiply both sides by 
its inverse and then get \(\vec{v} = 0\). But we do not want that therefore, \((A\vec{v} - \lambda I)\) 
must be linear transformation whose determinant is \(0\) which means that it can not ve invertible.

\begin{align*}    
    (A\vec{v} - \lambda I) \vec{v} &= \vec{0}\\
    \det (A\vec{v} - \lambda I)  &= 0
\end{align*}

\subsection{Characteristic Polynomial}

The polynomial which we get from the determinant \(\det (A\vec{v} - \lambda I)  = 0\) is called 
the characteristic polynomial of \(A\).

\[
    \mathcal{X}_A = \det (A\vec{v} - \lambda I) = 0
\]

\subsection{Theorems of Eigenvalues and Eigenvectors}

\subsubsection{Theorem I}

Given an eigenvalue \(\lambda\) of \(f\) and \(v_1, \dots, v_n\) being the eigenvectors of \(f\) 
with \(\lambda\) then \(v \in L(v_1, \dots, v_k) \backslash \{0\}\) is an eigenvector of 
\(f\) with \(\lambda\).
\vspace{\baselineskip}

\subsubsection{Theorem II}

For \(\lambda \in \Complex\) is \(Eig(f;\lambda):= \{c \in V | f(v) = \lambda v\}\) the \emph{Eigenspace} of 
\(f\) with \(\lambda\) and, it is a subspace of \(V\).

\subsubsection{Theorem III}

For \(\lambda \ne \gamma \quad Eig(f;\lambda) \cap Eig(f;\gamma) = \{0\}\).

\subsubsection{Theorem IV}

Eigenvectors with different eigenvalues are linearly independent.

\subsubsection{Theorem V}

\(Eig(f;\lambda) = ker(A - \lambda E)\)

\subsubsection{Theorem VI}

Given the following eigenvalues\(\lambda_1, \dots, \lambda_n\), then the 
determinant of the matrix \(A\) is given by 

\[
    \det(A) = \prod_{i = 1}^{n} \lambda_i
\]

\textbf{Proof:}

Given the eigenvalues \(\lambda_1, \dots, \lambda_n\) are the roots of the characteristic polynomial

\begin{align*}
    \det(A - \lambda I) = p (\lambda) &= (-1)^{n}(\lambda - \lambda_1)(\lambda - \lambda_2)\cdots\\
                                      &= (-1)(\lambda - \lambda_1)(-1)(\lambda - \lambda_2)\cdots\\     
                                      &= (\lambda - \lambda_1)(\lambda - \lambda_2)\cdots                       
\end{align*}

Now, by setting \(\lambda = 0\) we get 

\[
    \det(A) = \prod_{i = 1}^{n} \lambda_i
\]

Or by using the definition of the diagonal matrix:

\begin{align*}
    A &= XDX^{-1} \\
   \det(A) &= \det(XDX^{-1})\\
   \det(A) &= \det(X)\det(D)\det(X^{-1})\\
   \det(A) &= \det(D) = \prod_{i = 1}^{n} \lambda_i
\end{align*}

\QED

\subsubsection{Theorem VII}

For all linear isometric map \(f\) with some eigenvalue \(\lambda\) it applies that

\[
    \|x\| = \|f(x)\| = \|\lambda x\| = \|\lambda\|\|x\|
\]

Also, \(\lambda = 1\).
\vspace{\baselineskip}

An example for this are rotation matrices which map to the same space and preserve the length and 
whose eigenvalue are complex with a modulus of length of one.

\subsubsection{Theorem VIII}

For some eigenvector \(x\) with an eigenvalue \(\lambda\) of \(A\). It applies that

\[
    A^{k}x = \lambda^{k}x
\]


\subsection{How to find the Eigenvectors and Eigenvalues}

To find the eigenvalues and eigenvectors of a square matrix \(A\), we solve the eigenvalue equation:
\vspace{\baselineskip}

\textbf{1. Form the characteristic equation:}

Rewrite the equation \(A\vec{v} = \lambda\vec{v}\) as 
\((A - \lambda I)\vec{v} = \vec{0}\), where \(\lambda I\) is the identity matrix times \(\lambda\) because 
this matrix encodes the multiplication by some scalar. 
    
To have a non-trivial solution \(0\) for \(\vec{v}\), the matrix \((A - \lambda I)\) must be 
singular like said before, which means its determinant must be zero because otherwise \(\vec{v}\) 
would be 0. Thus, we have the characteristic equation:

\[
    \det(A - \lambda I) = 0
\]

\textbf{2. Solve for the eigenvalues:}

Solve the characteristic equation for \(\lambda\). The solutions 
\(\lambda_1, \lambda_2, \dots, \lambda_n\) are the eigenvalues of the matrix \(A\).
\vspace{\baselineskip}

\textbf{3. Find the eigenvectors:}

For each eigenvalue \(\lambda_i\), substitute it back into the equation 
\((A - \lambda_i I)\vec{v} = \vec{0}\) and solve for the vector \(\vec{v}\). The non-zero 
solutions for \(\vec{v}\) are the eigenvectors corresponding to the eigenvalue 
\(\lambda_i\). Note that we are going to get infinite solutions thus, we have to write 
our vector with dependence on the free 
parameters.
\vspace{\baselineskip}

\textbf{Example:}
\vspace{\baselineskip}

Find the eigenvalues and vectors of 

\[
    A = \begin{pmatrix}
        1 & 1 \\
        4 & 1 \\
    \end{pmatrix}
\]

Let us build the characteristic equation and solve it

\[
    \det 
    \begin{pmatrix}
        1 - \lambda & 1 \\
        4 & 1 - \lambda \\
    \end{pmatrix}
    = 0
\]

\begin{align*}
    (1 - \lambda)^2 - 4 &= 0 \\
    1 - 2\lambda + \lambda^2 - 4 &= 0 \\
    \lambda^2 - 2\lambda - 3 &= 0 \\
    (\lambda - 3) (\lambda + 1) &= 0 
\end{align*}

We have got the eigenvalues \(\lambda_1 = 3\) and \(\lambda_2 = -1\). Now let us continue by 
substituting our eigenvalues in the original equation.

\begin{align*}
    \begin{pmatrix} 1 - \lambda_2 & 1 \\ 4 & 1 - \lambda-2 \\ \end{pmatrix} &= \vec{0} \\
    \begin{pmatrix} 1 - 2 & 1 \\ 4 & 1 - 2 \\ \end{pmatrix} &= \vec{0} \\
    \begin{pmatrix} 2 & 1 \\ 4 & 2 \\ \end{pmatrix} &= \vec{0} 
\end{align*}

Using Gaussian elimination we get: 

\[
    \begin{pmatrix} 
        2 & 1 \\ 
        0 & 0 \\ 
    \end{pmatrix} 
\]

Thus, let \(v_2 = -2\) and \(v_1 = \frac{2}{2} = 1\). This give us the first eigenvector and all of it 
scalar versions.

\[
    e_1 = \left\{ c \begin{pmatrix} 1 \\ -2 \end{pmatrix} | c \in \Reals\right\}
\]

The process for the other eigenvalue is exactly the same.

\subsubsection{Complex Eigenvalues}

This is a special case of \emph{eigenvalues} which are not as easy to visualize.
\vspace{\baselineskip}

\textbf{Example:}

Find the eigenvalues and vectors of

\[
    A = \begin{pmatrix}
        0 & 1 \\
        -1 & 0 
    \end{pmatrix}
\]

\[
    \det\left( 
    \begin{pmatrix}
        -\lambda & 1 \\
        -1 & -\lambda
    \end{pmatrix}
    \right) = 0
\]

\[
    \lambda^2 + 1 = 0 
\]

\[
    \lambda = \pm i
\]

Now the corresponding eigenvectors.
\vspace{\baselineskip}

For \(\lambda = i\)

\[     
    \begin{pmatrix}
        -i & 1 \\
        -1 & -i
    \end{pmatrix}
    \rightarrow
    \begin{pmatrix}
        1 & i \\
        0 & 0
    \end{pmatrix} 
\]

Therefore, our first eigenvector is 

\[  
    c
    \begin{pmatrix}
        -i \\
        1        
    \end{pmatrix}
\]

For \(\lambda = -i\)

\[     
    \begin{pmatrix}
        i & 1 \\
        -1 & i
    \end{pmatrix}
    \rightarrow
    \begin{pmatrix}
        1 & -i \\
        0 & 0
    \end{pmatrix} 
\]

Therefore, the second eigenvector is 

\[  
    c
    \begin{pmatrix}
        i \\
        1        
    \end{pmatrix}
\]

\subsection{Similar Matrices}

\(A \sim B\) if there is some \(X\) such that 

\[
    A = XDX^{-1} \iff X^{-1}AX = D
\]

If our matrices are really similar then:

\begin{align*}
    \det(A - \lambda I) &= \det(XDX^{-1} - \lambda I)\\
    \det(A - \lambda I) &= \det(XDX^{-1} - \lambda X I X^{-1})\\
    \det(A - \lambda I) &= \det(X[D - \lambda I] X^{-1})\\
    \det(A - \lambda I) &= \det(X)\det(D - \lambda X) \det(X^{-1})\\
    \det(A - \lambda I) &= \det(D - \lambda X) 
\end{align*}

Thus, similar matrices have the same eigenvalues with multiplicity.

\subsection{Diagonal Matrices}

A \emph{diagonal matrix} is matrix of dimensions \(n \times n\) where only the entries of the main diagonal 
are not zero. This kind of matrices make a lot of the computations in linear algebra trivial. Therefore, 
if our matrix is not diagonal we are interested in finding a way of transforming the original matrix 
into a diagonal matrix.
\vspace{\baselineskip} 

A matrix is diagonalizable if and only if the eigenvectors of it are linear independent.

\subsubsection{Theorem I}

Given a matrix \(A \in \Complex^{n \times n}\) with \(n\) distinct eigenvalues then is \(A\)
diagonalizable. But this does not guaranty the diagonalizibility for \(\Reals\).

\subsubsection{Theorem II}

Given a matrix \(A \in \Complex^{n \times n}\) with \(\lambda\) as eigenvalue. The multiple occurrences 
of the zero \(\lambda\) of \(\mathcal{X}_A\) is called \emph{algebraic multiplicity} \(a(\lambda)\). 
Continuing, \(g(\lambda):= dim(Eig(A;\lambda))\) is the \emph{geometrical multiplicity} of \(\lambda\).
\vspace{\baselineskip}

Also, for an eigenvalue \(\lambda\) with \(a(\lambda) > g(\lambda)\) then is \(A\) not diagonalizable.

\subsubsection{Theorem III}

Eigenvalues of real symmetric matrices are real and also they respective eigenvectors.

\subsubsection{Theorem IV}

Given a symmetric matrix \(A \in \Reals^{n \times n}\) with \(\lambda \ne \mu \) as two eigenvalues of \(A\) 
with eigenvectors \(v\) and \(w\). Then \(v\) and \(w\) are \emph{orthogonal} to each other.
\vspace{\baselineskip}

\textbf{Proof:}

\[
    \langle w, Av \rangle = w^{T} Av = (A^T w)^T v = (Aw)^T v = \langle Aw, v\rangle
\]

Therefore, \(\lambda \langle v, w \rangle = \langle Av, w \rangle = \langle v, Aw\rangle 
= \mu \langle v, w \rangle\). Because \(\mu \ne \lambda\) and \(\lambda - \mu \langle v, w\rangle = 0\).
The vectors must be orthogonal.

\QED

\subsubsection{Matrix Diagonalization}

Diagonalizing a matrix involves finding a diagonal matrix that is similar to the given matrix. 
A square matrix \(A\) is diagonalizable if there exists an invertible matrix \(X\) made of eigenvectors of 
\(A\) such that \(X^{-1}AX = D\), where \(D\) is a diagonal matrix made of eigenvalues.
\vspace{\baselineskip}

The order of the eigenvalue and eigenvectors matters if you choose 
\(\lambda_1, \lambda_2, \dots, \lambda_n\) then the order of the eigenvectors 
\(v_1, v_2, \dots, v_n\) must also correspond. This process only works if \(A\) 
has unique eigenvalues or if they are duplicates they have to be linearly independent.
\vspace{\baselineskip}

The process of diagonalization is as follows:
\vspace{\baselineskip}

\textbf{1. Find the eigenvalues and eigenvectors of \(A\).}
\vspace{\baselineskip}

\textbf{2. Form the matrix \(X\):}

Create a matrix \(X\) whose columns are the linearly independent eigenvectors of \(A\).
\vspace{\baselineskip}

\textbf{3. Form the diagonal matrix \(D\):} 

Create a diagonal matrix \(D\) whose diagonal entries are the eigenvalues of \(A\), corresponding to 
the order of the eigenvectors in \(P\). That is, if the \(i\)-th column of \(P\) is the eigenvector 
corresponding to the eigenvalue \(\lambda_i\), then the \(i\)-th diagonal entry of \(D\) is \(\lambda_i\).
\vspace{\baselineskip}

\textbf{4. Verify the diagonalization:}
    
Check that \(X^{-1}AX = D\).
\vspace{\baselineskip}

A matrix \(A\) is diagonalizable if and only if it has \(n\) linearly independent eigenvectors, where 
\(n\) is the size of the matrix.
\vspace{\baselineskip}

\textbf{Example: }
\vspace{\baselineskip}

Consider the matrix

\[
    A = \begin{pmatrix}
    -3 & -4 \\
    5 & 6
    \end{pmatrix}
\]


The characteristic equation is

\[
    \det(A - \lambda I) 
    = \det 
    \begin{pmatrix}
            -3 - \lambda & -4 \\
        5 & 6 - \lambda
    \end{pmatrix} 
    = (-3 - \lambda)(6 - \lambda) + 20 = \lambda^2 - 3\lambda + 2 = 0    
\]
\[
    (\lambda - 1)(\lambda - 2)
\]

Solving for \(\lambda\), we get \(\lambda_1 = 1\) and \(\lambda_2 = 2\).
\vspace{\baselineskip}

For \(\lambda_1 = 1\):

\[
   (A - I)\vec{v} = \begin{pmatrix}
    -4 & -4 \\
    5 & 5
    \end{pmatrix} \begin{pmatrix}
    x \\
    y
    \end{pmatrix} = \begin{pmatrix}
    0 \\
    0
    \end{pmatrix}
\]

Note that this is telling us that \(x = -y\). Now choose \(x = -1\) and \(y = 1\), which gives us 
the vector 

\[
    \vec{x} = \begin{pmatrix}
        -1 // 1
    \end{pmatrix}
\]

For \(\lambda_2 = 2\) the process is similar, so we are going to skip it. We get the vector

\[
    \vec{y} = \begin{pmatrix}
        -\frac{4}{5} \\ 1
    \end{pmatrix}
\]

Now we will complete the process by writing \(D, X, \text{ and } X^{-1}\).

\[
    D = \begin{pmatrix}
        1 & 0 \\
        0 & 2
    \end{pmatrix}
    X = \begin{pmatrix}
        -1  & -\frac{4}{5} \\
        1   &   1 
    \end{pmatrix}
    X^{1} = \begin{pmatrix}
        -1  & \frac{4}{5} \\
        -1   &   -1 
    \end{pmatrix}
\]






