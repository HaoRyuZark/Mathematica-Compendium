\newpage
\section{Over/Underdetermined Systems of Equations}

Let us start with a little motivation for this chapter. Imagine that you have collected some data points, 
and you want to relate them via a linear function \(Ax = (\alpha, \beta, \dots)^T\). This would probably 
not work because it will be impossible to connect all the points, but you could make an approximation 
which is the line \(\alpha x_1 + \beta x_2 + \dots\) that is the closest to all the points in your data 
set. Thus, we will find an approximation 
\(b\) that is close to the image of \(A\) (\(Im(A)\)).

For this purpose will use a theorem that says that the best approximation is the orthogonal projection of 
\(p_A (b)\). This projection can be found if there exist an orthonormal basis of the subspace we want to 
project our vector onto.

\subsection{Normal Equations}

Given a projection \(p_A (b)\) onto the subspace \(U\) generated by \(A = (a_1, \dots, a_n)\) then there 
exists an \(x \in \Reals^n\) such that 

\[
	p_{A}(b) = \sum_{k = 1}^{n} x_k a_k = Ax
\]

Also, because of the orthogonal property then

\begin{align*}
	b - p_{A}(b) \bot U &\implies b - Ax  \in U^{\bot}\\
						&\implies b - Ax \bot a_k \quad \forall k\\
						&\implies \langle a_k, b - Ax \rangle = 0 \quad \forall k\\
						&\iff A^{T} (b - Ax) = \vec{0}\\
						&\implies A^{T}b = Ax 
\end{align*}

This equation \(A^{T}b = Ax\) is called the \emph{normal equation}.

A \emph{normal equation} of a matrix \(A \in \Reals^{(m \times n)}\) has a solution. 
If \(rg(A) = n\) then 

\[
	x = {(A^T A)}^{-1} A^T b
\]

is a unique solution to the system. The term \({(A^T A)}^{-1}A^T\) is called the 
\emph{Generalized Inverse} of \(A\).

\textbf{Proof:}

The statement is true because of the existence of the orthogonal projection which give us a concrete basis. Also, 
the fact that \(rg(A) = n\) also implies that. And also because of the invertibility of \({(A^T A)}^{-1}\).

\QED

\subsection{Method of the Least Squares}

For a linear system of equations \(Ax=b\) with \(A \in \Reals^{(m \times n)}\), 
\(b \in \mathbb{B}^m\) and \(m \ge n\). For the case \(rg(A) = n\) we have

\[
	x_s = {(A^T A)}^{-1} A^T b
\]

and 

\[
	\| b - Ax_s \| = min_{z \in \Reals^n} \| b - Az \| 
\]

Here the vector \(z\) is called \emph{approximation solution via the method of the least squares}.

\textbf{Proof:}

We know that because of \(rg(A) = n\) our solution is unique. And that \(p_A(b) = Ax\) is our orthogonal 
projection. We can use Pythagoras theorem to

\begin{align*}
	\| b - Az \|^2 &= \| (b  - p_A (b))  + (p + p_A(b)) \|^2 \\
				&=  \| (b  - p_A (b))\|^2  +\|(p + p_A(b)) \|^2 \\
				&\ge  \| (b  - p_A (b))\|^2 \\
				&= \| b - Ax \|^2 
\end{align*}

Thus, \(p_A(b)\) gives us the smallest distance to the \(Img(A)\)

\QED

\textbf{Example I:}

\[
	x = 1, x = -1
\]

\[
	\| Ax - b \|^2 = (x + 1)^2 + (x - 1)^2 = 2x^2 + 2 
\]

Then \(\| Ax - b \|^2\) gives us the smallest distance for \(x = 0\).

\textbf{Example II:}

We can also take the last example and solve it with an alternative method.

\[
	A = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, x = (x), b = \begin{bmatrix} 1 \\ -1 \end{bmatrix}
\]

Let us use the normal equations

\[
	A^T Ax = A^T b
\]

\[
	(1, 1) 
	\begin{bmatrix} 
	1 \\ 
	1
	\end{bmatrix} x
	= 
	(1, 1)
	\begin{bmatrix} 
		1 \\ 
		-1
	\end{bmatrix} 
\]

\[
	\implies 2x = 0
\]

\textbf{Example 3:}

Given are 

\[
	x_1 = 1, x_2 = 2, x_1 + x_2 = -1
\]

with the corresponding matrix

\[
	\begin{bmatrix}
		1 & 0 \\
		0 & 1 \\
		1 & 1  
	\end{bmatrix}
\]

and solution vector 

\[
	b = \begin{bmatrix}
		1 \\
		2 \\
		-1
	\end{bmatrix}
\]

Now let us build the normal equation

\[
	\begin{bmatrix}
	1 & 0 & 1 \\
	0 & 1 & 1
	\end{bmatrix}
	\begin{bmatrix}
	1 & 0 \\
	0 & 1 \\
	1 & 1  
	\end{bmatrix}
	\vec{x}
	=
	\begin{bmatrix}
	1 & 0 & 1 \\
	0 & 1 & 1
	\end{bmatrix}
	\begin{bmatrix}
	1 \\
	2 \\
	-1
	\end{bmatrix}
\]

\[
	\begin{bmatrix}
	2 & 1 \\
	1 & 2
	\end{bmatrix}
	= 
	\begin{bmatrix}
	0 \\
	1
	\end{bmatrix}
\]

Now with Gaussian elimination we get \(x = (-\frac{1}{3}, \frac{2}{3})^T\). 

Finally, we can use the \(\vec{x}\) to solve \(Ax - b\)

\[
	\begin{bmatrix}
		1 & 0 \\
		0 & 1 \\
		1 & 1 \\
	\end{bmatrix}
	\begin{bmatrix}
		-\frac{1}{3} \\
		\frac{2}{3}
	\end{bmatrix}
	-
	\begin{bmatrix}
		1 \\
		2 \\
		-1
	\end{bmatrix}
	=
	\frac{4}{3}
	\begin{bmatrix}
	-1 \\
	-1 \\
	1
	\end{bmatrix}
\]

And therefore, \(\|Ax - b\|^2 = \frac{48}{9}\).

\subsection{Underdetermined Systems of Equations}

For a linear system \(Ax = b\) such that \(A \in \Reals^{m \times n}\), \(x\in \Reals^n\), 
\(b \in \Reals^m\) and \(m \le n\). For \(rg(A) = m\) we get the solution vector

\[
	x_s = A^T {(A A^T)}^{-1} b
\]

Also, \(x_s\) is the solution with the smallest length. 

\[
	\|x_s\| = min_{Ax = b} \|x\|,
\] 

and \(x_s\) is orthogonal to the homogenous system \(Az = 0\) which means

\[
	x_s \bot ker(A)
\]

Finally, we call for  \(rg(A) = m\), \(A^T {(A A^T)}^{-1}\) the \emph{Generalized Inverse} of \(A\).

\textbf{Proof:}

First, we show the invertibility of \(AA^T\) because this implies that \(x_s\) is defined.

Given \(x \in kern(AA^T)\). This implies that 

\[
	AA^T x = 0	
\] 

which implies 

\[
	x^T AA^T x = 0
\] 

because of \(x^T A = A^T x\) and this gives us 

\[
	\|A^T x\| = 0
\] 

therefore, \(x \in kern(A^T)\).

Now because of \(rg(A^T) = rg(A) = m\) we get \(dim(kern(A)^T) = 0\), thus \(x = \vec{0}\). 
Because of that \(kern(A A^T) = 0\) and \(rg(AA^T) = m\) thus, the invertibility is proven.

\(x_s\) is a solution of \(Ax = b\) because of 

\[
	Ax_s = A(A^T {(A A^T)}^{-1} b) = AA^T {(A A^T)}^{-1} b = b
\]

\(x_s \bot kern(A)\) because for \(z \in kern)(A)\)

\[
	\langle z, x_s\rangle = z^T A^T {(A A^T)}^{-1} b = \langle Az, {(A A^T)}^{-1} b \rangle = 0
\]

doe to \(Az = 0\).

Finally, thanks to the general solution theorem we know that \(x = x_s + z\). Using Pythagoras and by 
taking into consideration \(x_s \bot kern(A)\)

\[
	\| x\|^2 = \|x\|^2 + \|z\|^2 \ge \|x_s\|^2
\]

\textbf{Example:}

We have the equation \(2x_1 + 2x_2 - x_3 = 6\).

In this case the solution build a plane in \(\Reals^3\), thus the solution is the normal vector that 
cuts the plane. We build the hessian normal form.

\[
	\frac{1}{3} (2x_1 + 2x_2 - x_3) = 2
\]

whose solution is \(x = \frac{2}{3}(2, 2, -1)^T\).

Now with our method: \(A = (2, 2, -1)\), \(x = (x_1, x_2, x_3)\), \(b = 6\)

\[
	x = A^T {(AA^T)}^{-1} b =
	\begin{bmatrix}
	2\\
	2\\
	-1
	\end{bmatrix}
	{\left(
		(2,2,-1)
	\begin{bmatrix}
	2\\
	2\\
	-1		
	\end{bmatrix}
	\right)}^{-1}
	6
\]

\[
	x = \frac{2}{3}(2, 2, -1)^T
\]

