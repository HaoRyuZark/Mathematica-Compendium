\newpage
\section{Over/Underdetermined Systems of Equations}

Let us start with a little motivation for this chapter. Imagine that you have collected some data points, 
and you want to relate them via a linear function \(Ax = (\alpha, \beta, \dots)^T\). This would probably 
not work because it will be impossible to connect all the points, but you could make an approximation 
which is the line \(\alpha x_1 + \beta x_2 + \dots\) that is the closest to all the points in your data 
set. Thus, we will find an approximation 
\(b\) that is close to the image of \(A\) (\(Im(A)\)).
\vspace{\baselineskip}

For this purpose will use a theorem that says that the best approximation is the orthogonal projection of 
\(p_A (b)\). This projection can be found if there exist an orthonormal basis of the subspace we want to 
project our vector onto.

\subsection{Normal Equations}

Given a projection \(p_A (b)\) onto the subspace \(U\) generated by \(A = (a_1, \dots, a_n)\) then there 
exists an \(x \in \Reals^n\) such that 

\[
	p_{A}(b) = \sum_{k = 1}^{n} x_k a_k = Ax
\]

Also, because of the orthogonal property then

\begin{align*}
	b - p_{A}(b) \bot U &\implies b - Ax  \in U^{\bot}\\
						&\implies b - Ax \bot a_k \quad \forall k\\
						&\implies \langle a_k, b - Ax \rangle = 0 \quad \forall k\\
						&\iff A^{T} (b - Ax) = \vec{0}\\
						&\implies A^{T}b = Ax 
\end{align*}

This equation \(A^{T}b = Ax\) is called the \emph{normal equation}.
\vspace{\baselineskip}

A \emph{normal equation} of a matrix \(A \in \Reals^{(m \times n)}\) has a solution. 
If \(rg(A) = n\) then 

\[
	x = {(A^T A)}^{-1} A^T b
\]

is a unique solution to the system. The term \({(A^T A)}^{-1}A^T\) is called the 
\emph{Generalized Inverse} of \(A\).
\vspace{\baselineskip}

\textbf{Proof:}

The statement is true because of the existence of the orthogonal projection which give us a concrete basis. Also, 
the fact that \(rg(A) = n\) also implies that. And also because of the invertibility of \({(A^T A)}^{-1}\).

\QED

\subsection{Method of the Least Squares}

For a linear system of equations \(Ax=b\) with \(A \in \Reals^{(m \times n)}\), 
\(b \in \mathbb{B}^m\) and \(m \ge n\). For the case \(rg(A) = n\) we have

\[
	x_s = {(A^T A)}^{-1} A^T b
\]

and 

\[
	\| b - Ax_s \| = min_{z \in \Reals^n} \| b - Az \| 
\]

Here the vector \(z\) is called \emph{approximation solution via the method of the least squares}.
\vspace{\baselineskip}

\textbf{Proof:}

We know that because of \(rg(A) = n\) our solution is unique. And that \(p_A(b) = Ax\) is our orthogonal 
projection. We can use Pythagoras theorem to


\begin{align*}
	\| b - Az \|^2 &= \| (b  - p_A (b))  + (p + p_A(b)) \|^2 \\
				&=  \| (b  - p_A (b))\|^2  +\|(p + p_A(b)) \|^2 \\
				&\ge  \| (b  - p_A (b))\|^2 \\
				&= \| b - Ax \|^2 
\end{align*}

Thus, \(p_A(b)\) gives us the smallest distance to the \(Img(A)\)

\QED
\vspace{\baselineskip}

\textbf{Example I:}
\vspace{\baselineskip}

\[
	x = 1, x = -1
\]

\[
	\| Ax - b \|^2 = (x + 1)^2 + (x - 1)^2 = 2x^2 + 2 
\]

Then \(\| Ax - b \|^2\) gives us the smallest distance for \(x = 0\).
\vspace{\baselineskip}

\textbf{Example II:}
\vspace{\baselineskip}
We can also take the last example and solve it with an alternative method.

\[
	A = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, x = (x), b = \begin{pmatrix} 1 \\ -1 \end{pmatrix}
\]

Let us use the normal equations

\[
	A^T Ax = A^T b
\]

\[
	(1, 1) 
	\begin{pmatrix} 
	1 \\ 
	1
	\end{pmatrix} x
	= 
	(1, 1)
	\begin{pmatrix} 
		1 \\ 
		-1
	\end{pmatrix} 
\]

\[
	\implies 2x = 0
\]

\textbf{Example 3:}
\vspace{\baselineskip}

Given are 

\[
	x_1 = 1, x_2 = 2, x_1 + x_2 = -1
\]

with the corresponding matrix

\[
	\begin{pmatrix}
		1 & 0 \\
		0 & 1 \\
		1 & 1  
	\end{pmatrix}
\]

and solution vector 

\[
	b = \begin{pmatrix}
		1 \\
		2 \\
		-1
	\end{pmatrix}
\]

Now let us build the normal equation

\[
	\begin{pmatrix}
	1 & 0 & 1 \\
	0 & 1 & 1
	\end{pmatrix}
	\begin{pmatrix}
	1 & 0 \\
	0 & 1 \\
	1 & 1  
	\end{pmatrix}
	\vec{x}
	=
	\begin{pmatrix}
	1 & 0 & 1 \\
	0 & 1 & 1
	\end{pmatrix}
	\begin{pmatrix}
	1 \\
	2 \\
	-1
	\end{pmatrix}
\]

\[
	\begin{pmatrix}
	2 & 1 \\
	1 & 2
	\end{pmatrix}
	= 
	\begin{pmatrix}
	0 \\
	1
	\end{pmatrix}
\]

Now with Gaussian elimination we get \(x = (-\frac{1}{3}, \frac{2}{3})^T\). 
\vspace{\baselineskip}

Finally, we can use the \(\vec{x}\) to solve \(Ax - b\)

\[
	\begin{pmatrix}
		1 & 0 \\
		0 & 1 \\
		1 & 1 \\
	\end{pmatrix}
	\begin{pmatrix}
		-\frac{1}{3} \\
		\frac{2}{3}
	\end{pmatrix}
	-
	\begin{pmatrix}
		1 \\
		2 \\
		-1
	\end{pmatrix}
	=
	\frac{4}{3}
	\begin{pmatrix}
	-1 \\
	-1 \\
	1
	\end{pmatrix}
\]

And therefore, \(\|Ax - b\|^2 = \frac{48}{9}\).

\subsection{Underdetermined Systems of Equations}

For a linear system \(Ax = b\) such that \(A \in \Reals^{m \times n}\), \(x\in \Reals^n\), 
\(b \in \Reals^m\) and \(m \le n\). For \(rg(A) = m\) we get the solution vector

\[
	x_s = A^T {(A A^T)}^{-1} b
\]

Also, \(x_s\) is the solution with the smallest length. 

\[
	\|x_s\| = min_{Ax = b} \|x\|,
\] 

and \(x_s\) is orthogonal to the homogenous system \(Az = 0\) which means

\[
	x_s \bot ker(A)
\]

Finally, we call for  \(rg(A) = m\), \(A^T {(A A^T)}^{-1}\) the \emph{Generalized Inverse} of \(A\).
\vspace{\baselineskip}

\textbf{Proof:}

First, we show the invertibility of \(AA^T\) because this implies that \(x_s\) is defined.
\vspace{\baselineskip}

Given \(x \in kern(AA^T)\). This implies that 

\[
	AA^T x = 0	
\] 

which implies 

\[
	x^T AA^T x = 0
\] 

because of \(x^T A = A^T x\) and this gives us 

\[
	\|A^T x\| = 0
\] 

therefore, \(x \in kern(A^T)\).
\vspace{\baselineskip}

Now because of \(rg(A^T) = rg(A) = m\) we get \(dim(kern(A)^T) = 0\), thus \(x = \vec{0}\). 
Because of that \(kern(A A^T) = 0\) and \(rg(AA^T) = m\) thus, the invertibility is proven.
\vspace{\baselineskip}

\(x_s\) is a solution of \(Ax = b\) because of 

\[
	Ax_s = A(A^T {(A A^T)}^{-1} b) = AA^T {(A A^T)}^{-1} b = b
\]


\(x_s \bot kern(A)\) because for \(z \in kern)(A)\)

\[
	\langle z, x_s\rangle = z^T A^T {(A A^T)}^{-1} b = \langle Az, {(A A^T)}^{-1} b \rangle = 0
\]

doe to \(Az = 0\).
\vspace{\baselineskip}

Finally, thanks to the general solution theorem we know that \(x = x_s + z\). Using Pythagoras and by 
taking into consideration \(x_s \bot kern(A)\)

\[
	\| x\|^2 = \|x\|^2 + \|z\|^2 \ge \|x_s\|^2
\]


\textbf{Example:}
\vspace{\baselineskip}

We have the equation \(2x_1 + 2x_2 - x_3 = 6\).
\vspace{\baselineskip}

In this case the solution build a plane in \(\Reals^3\), thus the solution is the normal vector that 
cuts the plane. We build the hessian normal form.

\[
	\frac{1}{3} (2x_1 + 2x_2 - x_3) = 2
\]

whose solution is \(x = \frac{2}{3}(2, 2, -1)^T\).
\vspace{\baselineskip}

Now with our method: \(A = (2, 2, -1)\), \(x = (x_1, x_2, x_3)\), \(b = 6\)

\[
	x = A^T {(AA^T)}^{-1} b =
	\begin{pmatrix}
	2\\
	2\\
	-1
	\end{pmatrix}
	{\left(
		(2,2,-1)
	\begin{pmatrix}
	2\\
	2\\
	-1		
	\end{pmatrix}
	\right)}^{-1}
	6
\]

\[
	x = \frac{2}{3}(2, 2, -1)^T
\]

